#!/usr/bin/env perl
#
# Extracts URL from a HTML document and optionally applies a supplied
# Perl regular expression to the URL to search for matches. Run
# perldoc(1) on this script for additional documentation.

use strict;
use warnings;

use File::Basename qw(basename);
my $basename = basename($0);

my $output_tmpl = '%{url}\n';

use HTML::TokeParser ();
use LWP::UserAgent   ();
use URI              ();

END {
  # Report problems when writing to stdout (perldoc perlopentut)
  unless ( close(STDOUT) ) {
    warn "error: problem closing STDOUT: $!\n";
    exit 74;
  }
}

use Getopt::Std;
my %opts;
getopts 'h?tTb:o:m:', \%opts;

print_help() if exists $opts{h} or exists $opts{'?'} or @ARGV < 2;

# only include <a href="... tags by default for matching
# TODO configuration file for this specification?
my %elements = ( a => { href => 1 } );
my $allelements = 0;
my %attributes;

# TODO this interface may need some improvement for production use??
if ( exists $opts{m} ) {
  %elements = ();

  for my $pair ( split /\s*,\s*/, $opts{m} ) {
    my ( $element, @attributes ) = split /\s+/, $pair;
    next unless defined $element;
    if ( $element eq '*' ) {
      $allelements = 1;
    }

    if (@attributes) {
      $elements{$element} = { map { $_ => 1 } @attributes };
    }
  }

  if ($allelements) {
    # merge all wanted attributes into hash for quick lookup
    for my $attrs ( values %elements ) {
      %attributes = ( %attributes, %$attrs );
    }
    %elements = ();
  }
}

$output_tmpl = $opts{o} if exists $opts{o};
$opts{t} = 1 if $output_tmpl =~ m/ %{text} /x;

# fix backslashed characters to literal, add newline if no trailing
# whitespace found
$output_tmpl =~ s/(\\.)/qq{"$1"}/eeg;
$output_tmpl .= "\n" unless $output_tmpl =~ m/ \s$ /x;

my $base_uri = $opts{b} if exists $opts{b};

my $pattern  = shift;
my $document = shift;

print_urls( parse_document( get_document($document), $base_uri ) );

exit;

# gets document into something HTML::TokeParser can deal with
sub get_document {
  my $document = shift;
  return { document => $document, source => \*STDIN } if $document eq '-';

  my $source;
  $source->{document} = $document;

  my $target = URI->new($document);

  if ( $target->scheme ) {
    my $ua = LWP::UserAgent->new(
      env_proxy         => 1,
      keep_alive        => 0,
      timeout           => 30,
      protocols_allowed => [qw(http https ftp)],
      max_size          => 25000000
    );
    $ua->agent( $ENV{http_agent} ) if exists $ENV{http_agent};
    $ua->cookie_jar( { file => "$ENV{HOME}/.lwp-cookies" } );

    my $response = $ua->get( $target->canonical );
    if ( $response->is_success ) {
      $source->{base_uri} = $target->canonical->as_string;
      delete $source->{base_uri}
        unless defined $source->{base_uri}
        and length $source->{base_uri} >= 1;
      $source->{source} = \$response->content;
    } else {
      remark(
        'error',
        'could not fetch resource',
        { status => $response->code, errno => $response->message }
      );
      exit 102;
    }
  } else {
    # treat as filename as no scheme
    $source->{source} = $target->canonical->as_string;
  }

  return $source;
}

# pulls out URL from document
sub parse_document {
  my $source   = shift;
  my $base_uri = shift;

  my $p = HTML::TokeParser->new( $source->{source} );
  unless ($p) {
    remark(
      'error',
      'could not parse document',
      { source => $source->{document}, errno => $! }
    );
    exit 103;
  }

  my ( @found, $inbody, $seenbase, %seenurl );

  # command line base_uri wins if set
  $seenbase = 1 if defined $base_uri;

  while ( my $token = $p->get_tag ) {
    $inbody = 1 if $token->[0] eq 'body' or $token->[0] eq '/head';

    # skip closing tags
    # TODO does this match empty <img /> tags or not?
    next if $token->[0] =~ m{^/};

    # pull optional <base> from HTML header, use first <base>
    # element found
    if ( not $inbody and not $seenbase and $token->[0] eq 'base' ) {
      my $url = get_url_from_token( $token, 'href' );
      $base_uri = $url if $url;
      $seenbase = 1;
    }

    next unless $allelements or exists $elements{ $token->[0] };
    my $attr_ref = $allelements ? \%attributes : $elements{ $token->[0] };

    for my $attr ( keys %{ $token->[1] } ) {
      next unless exists $attr_ref->{$attr};

      my $url = get_url_from_token( $token, $attr );
      next unless $url;

      # TODO this does unique on "a href" or "img src" but not both...
      # need option to specify a broader duplicate check?
      next if exists $seenurl{"$token->[0].$attr.$url"};
      $seenurl{"$token->[0].$attr.$url"} = 1;

      my $result =
        { element => $token->[0], attribute => $attr, url => $url };

      # save text inside <a> with URL for reference, or any tag that has
      # a URL with the -T option
      if ( exists $opts{t} and $token->[0] eq 'a' ) {
        $result->{text} = $p->get_trimmed_text('/a');
      } elsif ( exists $opts{T} ) {
        my $closing_tag = '/' . $token->[0];
        $result->{text} = $p->get_trimmed_text($closing_tag);
      }
      delete $result->{text}
        unless defined $result->{text}
        and length $result->{text} >= 1;

      push @found, $result;
    }
  }

  # set base_uri from document URL if possible (allows command line or
  # <base> to take precedence)
  if ( not defined $base_uri and defined $source->{base_uri} ) {
    $base_uri = $source->{base_uri};
  }

  for my $result (@found) {
    my $uri;
    if ($base_uri) {
      $uri = URI->new_abs( $result->{url}, $base_uri )->canonical;
      $result->{base_uri} = $base_uri;
    } else {
      $uri = URI->new( $result->{url} )->canonical;
    }
    unless ($uri) {
      remark(
        'warning',
        'skipping URL as unable to canonicalize',
        { url => $result->{url} }
      );
      next;
    }
    $result->{url}    = $uri->as_string;
    $result->{source} = $source->{document};

    # TODO would be nice to include the "filename" here so can match the
    # last bit in the URL path before any ? or other URL stuff
  }

  return \@found;
}

sub get_url_from_token {
  my $token = shift;
  my $attr = shift || 'href';
  return
        unless exists $token->[1]{$attr}
    and defined $token->[1]{$attr}
    and length $token->[1]{$attr} >= 1;
  return $token->[1]{$attr};
}

# handles output via template
sub print_urls {
  my $urls = shift;
  unless (@$urls) {
    remark( 'error', 'no URL found', { source => $document } );
    exit 101;
  }

  for my $result (@$urls) {
    next unless $result->{url} =~ m/$pattern/o;

    my $str_out = $output_tmpl;
    $str_out =~
      s/ \Q%{\E (\w+) \Q}\E / defined $result->{$1} ? $result->{$1} : q{} /egx;
    print $str_out;
  }
}

sub remark {
  my $priority   = shift;
  my $message    = shift;
  my $attributes = shift;

  chomp $message;

  my $attr_str;
  if ($attributes) {
    $attr_str = join ', ',
      map { $attributes->{$_} ||= q{}; "$_=$attributes->{$_}" }
      sort keys %$attributes;
  }

  warn "$priority: $message", ( $attr_str ? ": $attr_str" : q{} ), "\n";
  return 1;
}

sub print_help {
  warn "Usage: $basename [opts] regex URL|file|-\n";
  warn "\nRun perldoc(1) on this script for additional documentation.\n\n";
  exit 64;
}

__DATA__

=head1 NAME

urlgrep - extract URL from HTML given as input

=head1 SYNOPSIS

Match any C<*.patch> files on the OpenBSD errata page and download the
matches by piping them to C<wget> via C<xargs>.

  $ urlgrep '.patch$' http://openbsd.org/errata.html | \
    xargs wget

List both regular links and image sources.

  $ urlgrep -m 'a href,img src' '^' http://www.apple.com/

Look for most any link attributes in the file C<example.html> and use
custom output template.

  $ urlgrep -o '%{element} %{attribute} %{url}' \
    -m '* href src' '^' example.html

=head1 DESCRIPTION

=head2 Overview

Provides a means to list URL in a given HTML document, limited by an
optional Perl regular expression. Useful when a specific (matchable) set
of documents are listed on a page, but there are too many false
positives to justify simply downloading all links from a page.

The HTML elements and attributes to be searched for can be changed
on the fly, and the output customized to suit subsequent use of
the results.

=head2 Normal Usage

  $ urlgrep [options] regex URL|file|-

See L<"OPTIONS"> for details on the command line switches supported.

The script will determine whether a URL, a file, or standard input
(via a filename of C<->) should be parsed as HTML for links. A Perl
regular expression must be specified to limit matches. Any matched URL
will be printed to standard output, and any errors or warnings to
standard error.

Use of a caching proxy is recommended, as numerous hits on an URL may be
required while fine tuning a regular expression. If possible, this
script will use a proxy specified by the standard Unix proxy environment
variables of C<http_proxy> and C<ftp_proxy>.

  $ export http_proxy=http://proxy:3128
  $ urlgrep ...

=head1 OPTIONS

This script currently supports the following command line switches:

=over 4

=item B<-h>, B<-?>

Prints a brief usage note about the script.

=item B<-b> I<base-url>

Specify custom base URL that will be used to qualify any relative links
in the document. This custom URL will override any C<base> element found
in the document, or the URL if a URL was specified to download instead
of a local file or standard input.

=item B<-m> I<specification>

Match custom elements and attributes instead of default match on only
C<href> attributes of C<a> elements. The I<specification> is a comma
separated list of element attribute pairs, where a single element is
followed by one or more attribute names. To match all elements, use an
element name of C<*>.

=over 4

=item Default specification.

  -m 'a href'

=item Match image sources.

  -m 'img src'

=item Match both images and regular links.

  -m 'a href,img src'

=item Match attributes under any element.

  -m '* href src'

=back

=item B<-t> and B<-T>

These options control the saving of text inside tags with URL, in
conjunction with the C<%{text}> output macro. The B<-t> option enables
saving of text inside C<a href> tags, and is enabled by default if
C<%{text}> appears in the output template.

The B<-T> option enables saving of text inside any tag that a URL has
been found for. This might support newer linking conventions where the
C<href> or possibly C<xlink:href> attribute may be applied to most any
element, instead of just C<a> and a few others.

=item B<-o> I<template>

By default, each URL is printed on a different line via a template of
C<%{url}\n>. This can be changed from the command line with the B<-o>
option. The I<template> will expand any backslashed single characters,
allowing tabs and similar characters in the output, and a newline will
be appended to the template if the template does not end in whitespace.

  -o '%{url}\t%{text}'

Macros that can be enclosed inside C<%{}> statements include the
following.

  url       - URL in question.

  element   - Name of the element from which the URL is from,
              typically 'a'.
  attribute - Name of the attribute from which the URL is from,
              usually 'href'.

  source    - URL or filename of original document.
  text      - Text inside any C<a href> element, if any.

=back

=head1 EXAMPLES

=over 4

=item C<quoteurl>

To list URL followed by the text contained by the link, use the
following shell script wrapper.

  #!/bin/sh
  if [ -z "$1" ]; then
    echo "usage: `basename $0` regular-expression URL"
    exit 100
  fi
  urlgrep -o '%{url} "%{text}"' $1 $2

This allows quick matches of links to forward to others, for example via
the C<pbcopy> utility on Mac OS X to move the link and text to the
clipboard, in this case a random C<http> link from C<memepool.com>.

  $ quoteurl '^http://' http://www.memepool.com | randline | pbcopy

=item B<MySQL download problems>

This script was inspired from the annoying MySQL download process, where
microscopic text links must be followed through a maze of mirror sites,
and at some point the URL edited to get to a index page on a mirror
site. From this page, a slew of hard to click links must be followed,
which becomes tedious when attempting to download all the C<i386.rpm>
files, but none of the many other files listed.

  $ urlgrep '.(i386|src).rpm$' $SOME_MYSQL_MIRROR | \
    xargs wget

=back

=head1 BUGS

=head2 Reporting Bugs

Newer versions of this script may be available from:

http://github.com/thrig/sial.org-scripts/tree/master

If the bug is in the latest version, send a report to the author.
Patches that fix problems or add new features are welcome.

=head2 Known Issues

The B<-t> and B<-T> options may exclude URL in rare cases such as the
following where a URL is embedded inside another URL. This would be a
limitation of the current HTML processing module that could be fixed via
the use of a more featureful HTML module.

  <a href="/"> text <a href="#subsection"> more text </a> </a>

See the source for C<TODO> notes on ideas for improvements.

=head1 SEE ALSO

perl(1), perlre(1), HTML::TokeParser, LWP::UserAgent, URI

=head1 AUTHOR

thrig - Jeremy Mates (cpan:JMATES) C<< <jmates at cpan.org> >>

=head1 COPYRIGHT

Copyright (c) 2015 Jeremy Mates

     Permission to use, copy, modify, and/or distribute this software for
     any purpose with or without fee is hereby granted, provided that the
     above copyright notice and this permission notice appear in all
     copies.

     THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL
     WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED
     WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE
     AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL
     DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA
     OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER
     TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
     PERFORMANCE OF THIS SOFTWARE.

=cut
