#!/usr/bin/env perl
#
# Extracts URL from a HTML document and optionally applies a supplied
# Perl regular expression to the URL to search for matches. Run
# perldoc(1) on this script for additional documentation.

use strict;
use warnings;

use File::Basename qw(basename);
my $basename = basename($0);

my $output_tmpl = '%{url}\n';

use HTML::TokeParser ();
use LWP::UserAgent   ();
use URI              ();

END {
  # Report problems when writing to stdout (perldoc perlopentut)
  unless ( close(STDOUT) ) {
    warn "error: problem closing STDOUT: $!\n";
    exit 74;
  }
}

use Getopt::Std;
my %opts;
getopts 'h?tTb:o:m:', \%opts;

print_help() if exists $opts{h} or exists $opts{'?'} or @ARGV < 2;

# only include <a href="... tags by default for matching
# TODO configuration file for this specification?
my %elements = ( a => { href => 1 } );
my $allelements = 0;
my %attributes;

# TODO this interface may need some improvement for production use??
if ( exists $opts{m} ) {
  %elements = ();

  for my $pair ( split /\s*,\s*/, $opts{m} ) {
    my ( $element, @attributes ) = split /\s+/, $pair;
    next unless defined $element;
    if ( $element eq '*' ) {
      $allelements = 1;
    }

    if (@attributes) {
      $elements{$element} = { map { $_ => 1 } @attributes };
    }
  }

  if ($allelements) {
    # merge all wanted attributes into hash for quick lookup
    for my $attrs ( values %elements ) {
      %attributes = ( %attributes, %$attrs );
    }
    %elements = ();
  }
}

$output_tmpl = $opts{o} if exists $opts{o};
$opts{t} = 1 if $output_tmpl =~ m/ %{text} /x;

# fix backslashed characters to literal, add newline if no trailing
# whitespace found
$output_tmpl =~ s/(\\.)/qq{"$1"}/eeg;
$output_tmpl .= "\n" unless $output_tmpl =~ m/ \s$ /x;

my $base_uri = $opts{b} if exists $opts{b};

my $pattern  = shift;
my $document = shift;

print_urls( parse_document( get_document($document), $base_uri ) );

exit;

# gets document into something HTML::TokeParser can deal with
sub get_document {
  my $document = shift;
  return { document => $document, source => \*STDIN } if $document eq '-';

  my $source;
  $source->{document} = $document;

  my $target = URI->new($document);

  if ( $target->scheme ) {
    my $ua = LWP::UserAgent->new(
      env_proxy         => 1,
      keep_alive        => 0,
      timeout           => 30,
      protocols_allowed => [qw(http https ftp)],
      max_size          => 25000000
    );
    $ua->agent( $ENV{http_agent} ) if exists $ENV{http_agent};
    $ua->cookie_jar( { file => "$ENV{HOME}/.lwp-cookies" } );

    my $response = $ua->get( $target->canonical );
    if ( $response->is_success ) {
      $source->{base_uri} = $target->canonical->as_string;
      delete $source->{base_uri}
        unless defined $source->{base_uri}
        and length $source->{base_uri} >= 1;
      $source->{source} = \$response->content;
    } else {
      remark(
        'error',
        'could not fetch resource',
        { status => $response->code, errno => $response->message }
      );
      exit 102;
    }
  } else {
    # treat as filename as no scheme
    $source->{source} = $target->canonical->as_string;
  }

  return $source;
}

# pulls out URL from document
sub parse_document {
  my $source   = shift;
  my $base_uri = shift;

  my $p = HTML::TokeParser->new( $source->{source} );
  unless ($p) {
    remark(
      'error',
      'could not parse document',
      { source => $source->{document}, errno => $! }
    );
    exit 103;
  }

  my ( @found, $inbody, $seenbase, %seenurl );

  # command line base_uri wins if set
  $seenbase = 1 if defined $base_uri;

  while ( my $token = $p->get_tag ) {
    $inbody = 1 if $token->[0] eq 'body' or $token->[0] eq '/head';

    # skip closing tags
    # TODO does this match empty <img /> tags or not?
    next if $token->[0] =~ m{^/};

    # pull optional <base> from HTML header, use first <base>
    # element found
    if ( not $inbody and not $seenbase and $token->[0] eq 'base' ) {
      my $url = get_url_from_token( $token, 'href' );
      $base_uri = $url if $url;
      $seenbase = 1;
    }

    next unless $allelements or exists $elements{ $token->[0] };
    my $attr_ref = $allelements ? \%attributes : $elements{ $token->[0] };

    for my $attr ( keys %{ $token->[1] } ) {
      next unless exists $attr_ref->{$attr};

      my $url = get_url_from_token( $token, $attr );
      next unless $url;

      # TODO this does unique on "a href" or "img src" but not both...
      # need option to specify a broader duplicate check?
      next if exists $seenurl{"$token->[0].$attr.$url"};
      $seenurl{"$token->[0].$attr.$url"} = 1;

      my $result =
        { element => $token->[0], attribute => $attr, url => $url };

      # save text inside <a> with URL for reference, or any tag that has
      # a URL with the -T option
      if ( exists $opts{t} and $token->[0] eq 'a' ) {
        $result->{text} = $p->get_trimmed_text('/a');
      } elsif ( exists $opts{T} ) {
        my $closing_tag = '/' . $token->[0];
        $result->{text} = $p->get_trimmed_text($closing_tag);
      }
      delete $result->{text}
        unless defined $result->{text}
        and length $result->{text} >= 1;

      push @found, $result;
    }
  }

  # set base_uri from document URL if possible (allows command line or
  # <base> to take precedence)
  if ( not defined $base_uri and defined $source->{base_uri} ) {
    $base_uri = $source->{base_uri};
  }

  for my $result (@found) {
    my $uri;
    if ($base_uri) {
      $uri = URI->new_abs( $result->{url}, $base_uri )->canonical;
      $result->{base_uri} = $base_uri;
    } else {
      $uri = URI->new( $result->{url} )->canonical;
    }
    unless ($uri) {
      remark(
        'warning',
        'skipping URL as unable to canonicalize',
        { url => $result->{url} }
      );
      next;
    }
    $result->{url}    = $uri->as_string;
    $result->{source} = $source->{document};

    # TODO would be nice to include the "filename" here so can match the
    # last bit in the URL path before any ? or other URL stuff
  }

  return \@found;
}

sub get_url_from_token {
  my $token = shift;
  my $attr = shift || 'href';
  return
        unless exists $token->[1]{$attr}
    and defined $token->[1]{$attr}
    and length $token->[1]{$attr} >= 1;
  return $token->[1]{$attr};
}

# handles output via template
sub print_urls {
  my $urls = shift;
  unless (@$urls) {
    remark( 'error', 'no URL found', { source => $document } );
    exit 101;
  }

  for my $result (@$urls) {
    next unless $result->{url} =~ m/$pattern/o;

    my $str_out = $output_tmpl;
    $str_out =~
      s/ \Q%{\E (\w+) \Q}\E / defined $result->{$1} ? $result->{$1} : q{} /egx;
    print $str_out;
  }
}

sub remark {
  my $priority   = shift;
  my $message    = shift;
  my $attributes = shift;

  chomp $message;

  my $attr_str;
  if ($attributes) {
    $attr_str = join ', ',
      map { $attributes->{$_} ||= q{}; "$_=$attributes->{$_}" }
      sort keys %$attributes;
  }

  warn "$priority: $message", ( $attr_str ? ": $attr_str" : q{} ), "\n";
  return 1;
}

sub print_help {
  warn "Usage: $basename [opts] regex URL|file|-\n";
  warn "\nRun perldoc(1) on this script for additional documentation.\n\n";
  exit 64;
}

__DATA__

=head1 NAME

urlgrep - extract URL from HTML given as input

=head1 SYNOPSIS

Match any C<*.patch> files on the OpenBSD errata page and download the
matches by piping them to C<wget> via C<xargs>.

  $ urlgrep '.patch$' http://openbsd.org/errata.html | \
    xargs wget

List both regular links and image sources.

  $ urlgrep -m 'a href,img src' '^' http://www.apple.com/

Look for most any link attributes in the file C<example.html> and use
custom output template.

  $ urlgrep -o '%{element} %{attribute} %{url}' \
    -m '* href src' '^' example.html

=head1 DESCRIPTION

=head2 Overview

Provides a means to list URL in a given HTML document, limited by an
optional Perl regular expression. Useful when a specific (matchable) set
of documents are listed on a page, but there are too many false
positives to justify simply downloading all links from a page.

The HTML elements and attributes to be searched for can be changed
on the fly, and the output customized to suit subsequent use of
the results.

=head2 Normal Usage

  $ urlgrep [options] regex URL|file|-

See L<"OPTIONS"> for details on the command line switches supported.

The script will determine whether a URL, a file, or standard input
(via a filename of C<->) should be parsed as HTML for links. A Perl
regular expression must be specified to limit matches. Any matched URL
will be printed to standard output, and any errors or warnings to
standard error.

Use of a caching proxy is recommended, as numerous hits on an URL may be
required while fine tuning a regular expression. If possible, this
script will use a proxy specified by the standard Unix proxy environment
variables of C<http_proxy> and C<ftp_proxy>.

  $ export http_proxy=http://proxy:3128
  $ urlgrep ...

=head1 OPTIONS

This script currently supports the following command line switches:

=over 4

=item B<-h>, B<-?>

Prints a brief usage note about the script.

=item B<-b> I<base-url>

Specify custom base URL that will be used to qualify any relative links
in the document. This custom URL will override any C<base> element found
in the document, or the URL if a URL was specified to download instead
of a local file or standard input.

=item B<-m> I<specification>

Match custom elements and attributes instead of default match on only
C<href> attributes of C<a> elements. The I<specification> is a comma
separated list of element attribute pairs, where a single element is
followed by one or more attribute names. To match all elements, use an
element name of C<*>.

=over 4

=item Default specification.

  -m 'a href'

=item Match image sources.

  -m 'img src'

=item Match both images and regular links.

  -m 'a href,img src'

=item Match attributes under any element.

  -m '* href src'

=back

=item B<-t> and B<-T>

These options control the saving of text inside tags with URL, in
conjunction with the C<%{text}> output macro. The B<-t> option enables
saving of text inside C<a href> tags, and is enabled by default if
C<%{text}> appears in the output template.

The B<-T> option enables saving of text inside any tag that a URL has
been found for. This might support newer linking conventions where the
C<href> or possibly C<xlink:href> attribute may be applied to most any
element, instead of just C<a> and a few others.

=item B<-o> I<template>

By default, each URL is printed on a different line via a template of
C<%{url}\n>. This can be changed from the command line with the B<-o>
option. The I<template> will expand any backslashed single characters,
allowing tabs and similar characters in the output, and a newline will
be appended to the template if the template does not end in whitespace.

  -o '%{url}\t%{text}'

Macros that can be enclosed inside C<%{}> statements include the
following.

  url       - URL in question.

  element   - Name of the element from which the URL is from,
              typically 'a'.
  attribute - Name of the attribute from which the URL is from,
              usually 'href'.

  source    - URL or filename of original document.
  text      - Text inside any C<a href> element, if any.

=back

=head1 EXAMPLES

=over 4

=item C<quoteurl>

To list URL followed by the text contained by the link, use the
following shell script wrapper.

  #!/bin/sh
  if [ -z "$1" ]; then
    echo "usage: `basename $0` regular-expression URL"
    exit 100
  fi
  urlgrep -o '%{url} "%{text}"' $1 $2

This allows quick matches of links to forward to others, for example via
the C<pbcopy> utility on Mac OS X to move the link and text to the
clipboard, in this case a random C<http> link from C<memepool.com>.

  $ quoteurl '^http://' http://www.memepool.com | randline | pbcopy

=item B<MySQL download problems>

This script was inspired from the annoying MySQL download process, where
microscopic text links must be followed through a maze of mirror sites,
and at some point the URL edited to get to a index page on a mirror
site. From this page, a slew of hard to click links must be followed,
which becomes tedious when attempting to download all the C<i386.rpm>
files, but none of the many other files listed.

  $ urlgrep '.(i386|src).rpm$' $SOME_MYSQL_MIRROR | \
    xargs wget

=back

=head1 BUGS

=head2 Reporting Bugs

Newer versions of this script may be available from:

http://github.com/thrig/scripts/tree/master

If the bug is in the latest version, send a report to the author.
Patches that fix problems or add new features are welcome.

=head2 Known Issues

The B<-t> and B<-T> options may exclude URL in rare cases such as the
following where a URL is embedded inside another URL. This would be a
limitation of the current HTML processing module that could be fixed via
the use of a more featureful HTML module.

  <a href="/"> text <a href="#subsection"> more text </a> </a>

See the source for C<TODO> notes on ideas for improvements.

=head1 SEE ALSO

perl(1), perlre(1), HTML::TokeParser, LWP::UserAgent, URI

=head1 AUTHOR

Jeremy Mates

=cut
