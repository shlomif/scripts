#!/usr/bin/env perl
#
# Detects and deals with duplicate files with pre-index by size. Run
# perldoc(1) on this script for additional documentation.

use strict;
use warnings;

use Digest::SHA1 ();
use File::Find qw(find);
use File::Spec ();

END {
  # Report problems when writing to stdout (perldoc perlopentut)
  unless ( close(STDOUT) ) {
    warn "error: problem closing STDOUT: $!\n";
    exit 74;
  }
}

# this is reset for each file to avoid OO overhead on each file
my $digest = Digest::SHA1->new;

my %actions = (
  list   => \&handle_list,
  unlink => \&handle_unlink,
);
my @actions = sort keys %actions;

my %defaults;
@defaults{qw(action bydepth)} = qw(list 0);

use Getopt::Std;
my %opts;
getopts 'h?d0va:', \%opts;

print_help() if $opts{h} or $opts{'?'};

$defaults{action}  = $opts{a} if exists $opts{a};
$defaults{bydepth} = 1        if exists $opts{d};
$defaults{verbose} = 1        if exists $opts{v};

unless ( $defaults{action} =~ m/^ [a-z][A-Za-z0-9_-]{0,31} $/x ) {
  remark( 'error', 'invalid action name' );
  exit 78;
}
unless ( exists $actions{ $defaults{action} } ) {
  remark( 'error', 'no such action', { action => $defaults{action} } );
  exit 78;
}

# read from STDIN if no args left
chomp( @ARGV = <STDIN> ) unless @ARGV;

# only deal with regular files or directories
@ARGV = grep { -d $_ || -f _ } @ARGV;
print_help() unless @ARGV;

my ( %seen, %duplicates );

find( { wanted => \&find_dups, no_chdir => 1, bydepth => $defaults{bydepth} },
  @ARGV );

$actions{ $defaults{action} }->();

sub handle_list {
  for my $key ( keys %duplicates ) {
    next if @{ $duplicates{$key} } < 2;

    local $" = "\0" if exists $opts{0};

    print "@{ $duplicates{$key} }\n";
  }
}

sub handle_unlink {
  for my $key ( keys %duplicates ) {
    next if @{ $duplicates{$key} } < 2;

    # KLUGE log duplicates if verbose
    if ( exists $defaults{verbose} ) {
      local $" = "\0" if exists $opts{0};
      print "@{ $duplicates{$key} }\n";
    }

    my $wanted = @{ $duplicates{$key} };

    my $result = unlink @{ $duplicates{$key} }[ 1 .. $#{ $duplicates{$key} } ];

    if ( $wanted != $result ) {
      remark(
        'warning',
        'not all files unlinked',
        { expected => $wanted, deleted => $result }
      );
    }

  }
}

# File::Find handler to find duplicates
sub find_dups {
  return unless -f;

  my $size = -s;

  # empty files are a special case: duplicates make no sense for them
  return if $size == 0;

  unless ( exists $seen{$size} ) {
    $seen{$size}->{firsthit} = $_;
    return;
  }

  # generate checksum on first file hit of this size, as now have
  # second of same size
  if ( exists $seen{$size}->{firsthit} ) {
    my $checksum = gen_checksum( $seen{$size}->{firsthit} );
    push @{ $duplicates{"$checksum$size"} }, $seen{$size}->{firsthit}
     if defined $checksum;

    delete $seen{$size}->{firsthit};
  }

  my $checksum = gen_checksum($_);
  push @{ $duplicates{"$checksum$size"} }, $_ if defined $checksum;
}

sub gen_checksum {
  my $filename = shift;

  unless ( open FILE, "< $filename" ) {
    remark(
      'error',
      'could not open for checksum',
      { file => $filename, errno => $! }
    );
    return;
  }

  $digest->addfile(*FILE);
  close FILE;

  return $digest->b64digest;
}

sub remark {
  my $priority   = shift;
  my $message    = shift;
  my $attributes = shift;

  chomp $message;

  my $attr_str;
  if ($attributes) {
    $attr_str = join ', ',
     map { $attributes->{$_} ||= q{}; "$_=$attributes->{$_}" }
     sort keys %$attributes;
  }

  warn "$priority: $message", ( $attr_str ? ": $attr_str" : q{} ), "\n";
  return 1;
}

# a generic help blarb
sub print_help {
  warn <<"HELP";
Usage: $0 [options] [file-or-directory ..]

Detects and deals with duplicate files with pre-index by size.

Options:
  -h/-?  Display this message

  -d     Traverse files depth-first

  -a xx  Perform action with duplicates, default: $defaults{action}

Actions: @actions

  -0     Delimit duplicates with ASCII null character instead of space.

  -v     Become more verbose about certain operations.

Run perldoc(1) on this script for additional documentation.

HELP
  exit 64;
}

=head1 NAME

dupper - detects and deals with duplicate files with pre-index by size

=head1 SYNOPSIS

To get a list of duplicate files in a particular directory:

  $ dupper ~/public_html/images

=head1 DESCRIPTION

=head2 Overview

Finds duplicate files in one or more input files and directories.
Duplicates are matched though the use of SHA1 checksums; files are pre-
indexed by size for speed: checksums are only done when multiple files
share the same file size.

Handling for duplicates has yet to be written in this release.

=head2 Normal Usage

  $ dupper [options] [file-or-directory ..]

See L<"OPTIONS"> for details on the command line switches supported.

A list of files and directories to operate on should be specified on the
command line. Failing that, the script will attempt to read data from
standard input.

=head1 OPTIONS

This script currently supports the following command line switches:

=over 4

=item B<-h>, B<-?>

Prints a brief usage note about the script.

=item B<-d>

Perform depth-first traversal of files. This means deeper duplicates
will be listed first, which will change how actions affect the files.

=item B<-a> I<action>

Perform action I<action> on duplicates. Default is to list any
duplicates found, with unique duplicates grouped per line, with the
filepaths space separated. Other actions include code to unlink
duplicates past the first one found, and so forth.

=item B<-0>

Changes list output to use the ASCII null (000) character to delimit
duplicates. Different sets of duplicates will still be listed on
different lines.

=item B<-v>

Verbose logs of file unlinks, currently.

=back

=head1 BUGS

=head2 Reporting Bugs

Newer versions of this script may be available from:

http://github.com/thrig/sial.org-scripts/tree/master

If the bug is in the latest version, send a report to the author.
Patches that fix problems or add new features are welcome.

=head1 SEE ALSO

perl(1)

=head1 AUTHOR

thrig - Jeremy Mates (cpan:JMATES) C<< <jmates at cpan.org> >>

Pre-index on size concept adapted from code by Craig Reyenga.

=head1 COPYRIGHT

Copyright (c) 2015 Jeremy Mates

     Permission to use, copy, modify, and/or distribute this software for
     any purpose with or without fee is hereby granted, provided that the
     above copyright notice and this permission notice appear in all
     copies.

     THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL
     WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED
     WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE
     AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL
     DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA
     OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER
     TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
     PERFORMANCE OF THIS SOFTWARE.

=head1 SCRIPT CATEGORIES

Utilities

=cut
